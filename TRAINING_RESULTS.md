# Káº¿t Quáº£ Training vÃ  ÄÃ¡nh GiÃ¡ Model

## ğŸ“Š Káº¿t Quáº£ Sau Cáº£i Thiá»‡n

### Isolation Forest Model

**Tham sá»‘**: contamination = 0.25 (25%)

**Metrics**:
- **Precision**: 0.1392 (13.92%) â¬†ï¸ tá»« 12.5%
- **Recall**: 0.1419 (14.19%) â¬†ï¸ tá»« 2.58% (cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ!)
- **F1 Score**: 0.1406 (14.06%) â¬†ï¸ tá»« 4.28%
- **Accuracy**: 0.5744 (57.44%)

**Confusion Matrix**:
```
                Predicted
              Normal  Anomaly
Actual Normal     341     136
       Anomaly    133      22
```

**PhÃ¢n tÃ­ch**:
- âœ… Recall tÄƒng Ä‘Ã¡ng ká»ƒ: tá»« 2.58% lÃªn 14.19% (tÄƒng ~5.5 láº§n)
- âœ… F1 Score tÄƒng: tá»« 4.28% lÃªn 14.06% (tÄƒng ~3.3 láº§n)
- âš ï¸ Precision váº«n tháº¥p: 13.92% (nhiá»u false positives)
- âš ï¸ Accuracy giáº£m: tá»« 71.68% xuá»‘ng 57.44% (do phÃ¡t hiá»‡n nhiá»u anomalies hÆ¡n)

## ğŸ” So SÃ¡nh TrÆ°á»›c vÃ  Sau

| Metric | TrÆ°á»›c (contamination=0.05) | Sau (contamination=0.25) | Cáº£i thiá»‡n |
|--------|---------------------------|-------------------------|-----------|
| Precision | 12.50% | 13.92% | +1.42% |
| Recall | 2.58% | 14.19% | +11.61% â¬†ï¸â¬†ï¸ |
| F1 Score | 4.28% | 14.06% | +9.78% â¬†ï¸â¬†ï¸ |
| Accuracy | 71.68% | 57.44% | -14.24% |

**Nháº­n xÃ©t**:
- Model sau cáº£i thiá»‡n phÃ¡t hiá»‡n Ä‘Æ°á»£c nhiá»u anomalies hÆ¡n (Recall tÄƒng máº¡nh)
- Tuy nhiÃªn cÃ³ nhiá»u false positives hÆ¡n (Precision váº«n tháº¥p)
- Accuracy giáº£m nhÆ°ng Ä‘Ã¢y lÃ  Ä‘iá»u bÃ¬nh thÆ°á»ng khi model phÃ¡t hiá»‡n nhiá»u anomalies hÆ¡n

## ğŸ“ˆ Ground Truth Statistics

- **Tá»•ng sá»‘ samples**: 632
- **Anomalies thá»±c táº¿**: 155 (24.53%)
- **Normal thá»±c táº¿**: 477 (75.47%)
- **Anomalies phÃ¡t hiá»‡n**: 158 (25.00%)

## ğŸ¯ Äiá»ƒm Máº¡nh

1. âœ… **Recall cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ**: Model phÃ¡t hiá»‡n Ä‘Æ°á»£c nhiá»u anomalies hÆ¡n
2. âœ… **F1 Score tÄƒng**: CÃ¢n báº±ng tá»‘t hÆ¡n giá»¯a Precision vÃ  Recall
3. âœ… **Ground Truth tá»‘t hÆ¡n**: Sá»­ dá»¥ng code-based features
4. âœ… **Consistency**: Training vÃ  evaluation Ä‘á»u dÃ¹ng `code`

## âš ï¸ Äiá»ƒm Yáº¿u Cáº§n Cáº£i Thiá»‡n

1. **Precision tháº¥p** (13.92%): Nhiá»u false positives
   - **Giáº£i phÃ¡p**: Cáº£i thiá»‡n Ground Truth labels, thÃªm feature engineering

2. **Recall váº«n tháº¥p** (14.19%): Váº«n bá» sÃ³t nhiá»u anomalies
   - **Giáº£i phÃ¡p**: NÃ¢ng cáº¥p embedding model (CodeBERT), cáº£i thiá»‡n feature extraction

3. **Ground Truth cÃ³ thá»ƒ chÆ°a chÃ­nh xÃ¡c**: 
   - **Giáº£i phÃ¡p**: Review láº¡i cÃ¡ch táº¡o labels, cÃ³ thá»ƒ cáº§n manual labeling

## ğŸš€ CÃ¡c Cáº£i Thiá»‡n Tiáº¿p Theo

### Æ¯u tiÃªn Cao
1. **NÃ¢ng cáº¥p Embedding Model**: 
   - Thay `all-MiniLM-L6-v2` báº±ng CodeBERT hoáº·c GraphCodeBERT
   - Embeddings tá»‘t hÆ¡n cho code sáº½ cáº£i thiá»‡n cáº£ Precision vÃ  Recall

2. **Cáº£i thiá»‡n Ground Truth**:
   - Review láº¡i cÃ¡ch táº¡o labels
   - CÃ³ thá»ƒ cáº§n manual labeling má»™t pháº§n dá»¯ liá»‡u
   - Sá»­ dá»¥ng domain experts Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

3. **Feature Engineering**:
   - ThÃªm nhiá»u features tá»« code (AST, control flow, etc.)
   - Sá»­ dá»¥ng code analysis tools

### Æ¯u tiÃªn Trung bÃ¬nh
4. **Hyperparameter Tuning**:
   - Grid search cho contamination
   - Tune cÃ¡c tham sá»‘ khÃ¡c cá»§a Isolation Forest

5. **Ensemble Methods**:
   - Káº¿t há»£p Isolation Forest vÃ  Autoencoder
   - Voting hoáº·c weighted average

### Æ¯u tiÃªn Tháº¥p
6. **Cross-Validation**:
   - K-fold cross-validation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tá»‘t hÆ¡n
   - Stratified sampling

## ğŸ“ Káº¿t Luáº­n

Model Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» **Recall** vÃ  **F1 Score**, nhÆ°ng váº«n cáº§n cáº£i thiá»‡n vá» **Precision**. 

**Khuyáº¿n nghá»‹**:
1. Tiáº¿p tá»¥c cáº£i thiá»‡n Ground Truth labels
2. NÃ¢ng cáº¥p embedding model cho code
3. ThÃªm feature engineering
4. CÃ¢n nháº¯c ensemble methods

## ğŸ”— Files LiÃªn Quan

- `src/model_training.py`: Code training
- `src/evaluate_model.py`: Code evaluation vá»›i Ground Truth cáº£i thiá»‡n
- `models/trained_if.pkl`: Model Ä‘Ã£ train
- `data/processed/evaluation_results.csv`: Káº¿t quáº£ chi tiáº¿t



